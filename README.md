# 信息检索实验 —— TOP250 豆瓣电影短评爬虫与分析

## 项目概述

这个项目是我们这学期信息检索的实验，老师没做特殊的要求，只要求我们做个爬虫，对爬下来的信息进行一些处理，要做什么全看自己的兴趣，最好能和课上学的相关。因为去年看《Python3网络爬虫开发实战教程》记得有个爬电影的示例，以为是豆瓣电影的爬虫示例，所以本打算水一下改一下示例的代码交上去。只可惜后来发现示例是爬的猫眼电影，于是只能重新看一下爬虫教程改写豆瓣电影的爬虫。在写豆瓣电影爬虫过程中遇到了登录的滑动验证码和封号、封ip等问题，于是只能把《Python3网络爬虫开发实战教程》再看一遍才明白该怎么写...

其中项目目标参考了一位学长去年实验的难度和内容(毕竟只占20%成绩的实验我也不想写成课设...)，以及在Google豆瓣爬虫时看到的[豆瓣爬虫项目](https://github.com/iphysresearch/TOP250movie_douban#%E4%B8%80%E7%88%AC%E5%8F%96%E8%B1%86%E7%93%A3top250%E7%9A%84%E7%9F%AD%E8%AF%84%E6%95%B0%E6%8D%AE)

### 项目目标

本次我选择的实验题目是 豆瓣影评分析，包括但不限于爬取TOP250电影的所有短评以及电影的相关数据，运用神经网络对评论的评分进行预测和对情感进行分析，评论者的相关信息的可视化，电影类型对应的词云等，随着课程的推进为实验内容增加新的需求。

1. 爬取TOP250豆瓣电影所有短评，以及电影的相关信息，包括电影类型，上映时间，以及演员列表等信息；
2. 能够对短评的评分进行统计，以及某一类型电影下评论者的性别，年龄，加入豆瓣时间等，可视化表述统计结果，进而分析评论和电影类型的关系；
3. 能够分析指定电影指定时间段内的评论内容，包括对短评发表的频率统计，对短评内容的预处理，最终生成词云进行可视化表达；
4. 训练循环神经网络，学习短评的情感倾向和根据短评预测评分，测试数据使用再爬取的Mtime时光网的电影评论和评分。

## 项目构成

### Scrapy + Selenium 爬虫

#### 防反爬设置

和参考的爬虫项目一样，这个爬虫项目的大多数时间也花在了防反爬上。参考的爬虫项目反爬的设置没大看懂，于是自己根据书上的教程尝试防反爬策略，最重要的就是使用了selenium。其他的设置就是设置sleep时间等细枝末节，再就是log输出时最好设置成INFO，这样log内容可以更简洁，更容易看出自己爬虫的问题。

##### 踩过的坑

一开始尝试不使用scrapy框架直接爬取，这样可以爬取电影的具体信息，但只能爬取前10页短评，因为短评数量感觉有点少，所以尝试使用scrapy框架。
由于10页以后的短评必须要登录才可以爬取，看到书上有 CookiePool 和 ProxyPool，感觉这个对防反爬有帮助，于是看了一下教程将示例中的微博 CookiePool 改成[豆瓣CookiePool](https://github.com/DateBro/douban_movies_cookiepool)，其中做了对登录豆瓣时的滑动验证码的处理，通过selenium实现了自动完成验证码操作。
使用scrapy框架写好爬虫文件后，再将CookiePool和Proxy整合进来，发现一开始测试爬取几百条评论还能正常运行，但测试过三四次之后发现无法正常爬取。通过查看log文件和登录豆瓣发现由于爬取频率比较高被封号。分析之后感觉可能是因为六个账号多个ip登录被豆瓣后台检测出异常，毕竟挂代理登录豆瓣的时候会提示账号异地登录。
所以只能放弃已经废了一天弄完的CookiePool和ProxyPool，拜托同学帮忙注册一个豆瓣账号，尝试使用scrapy对接selenium完成爬虫。按照《Python3网络爬虫开发实战教程》的示例，将之前破解滑动验证码的部分整合到SeleniumMiddleware中。
这个时候测试，发现爬虫可以爬较长时间不被封ip，但测试的三次发现爬了大约12000条短评的时候就会异常，查看log文件发现error信息，感觉可能是被短时间封ip，因为重新启动爬虫仍然可以爬取短评。这个问题暂时没有想到好的解决方法，只能写一个shell脚本，每隔一段时间重新启动爬虫，而爬虫也在每次启动时读取一个txt文件中要爬取的电影范围。
经过测试发现，经过半天多就可以完成短评的爬取，由于豆瓣短评数量的限制，最终爬取的短评数量为 338143 条。

#### 爬取信息设置

TOP250_douban_movies 文件夹是 scrapy 项目文件夹，comments_spider.sh 是用于控制定时启动豆瓣短评爬虫的 shell 脚本。

spiders 文件夹下有三个spider，分别是 douban_movies.py —— 用于爬取 TOP250 电影榜的电影名称和电影链接，以及爬取每部电影的具体信息；

TOP250 排行榜要爬取的信息：
![.\images\Snipaste_2019-11-06_18-18-16.jpg]()

TOP250MovieInfoItem 设置:
![./images/Snipaste_2019-11-06_18-50-41.jpg]()

已爬取数据示例:
![./images/Snipaste_2019-11-06_18-53-33.jpg]()

每部电影要爬取的具体信息：
![./images/Snipaste_2019-11-06_18-45-57.jpg]()

![./images/Snipaste_2019-11-06_18-46-14.jpg]()

MovieInfoItem 设置：
![./images/Snipaste_2019-11-06_18-51-49.jpg]()

已爬取数据示例:
![./images/Snipaste_2019-11-06_18-54-34.jpg]()

对于每部电影具体信息的爬取，并没有和参考的TOP250豆瓣电影爬虫一样，有的信息是感觉太多自己用不上，有的信息是感觉不好分析可能增加太多工作量...

comments.py —— 用于爬取每部电影的短评，由于豆瓣的限制，最多只能爬取好评、中评、差评各 500 条；

每条短评要爬取的具体信息：
![./images/Snipaste_2019-11-06_18-58-42.jpg]()

CommentInfoItem 设置：
![./images/Snipaste_2019-11-06_18-59-05.jpg]()

本来一开始没想存储这条短评的网页链接，但一开始运行完爬虫发现只爬了27万多条短评，和一开始估计的37万相差过多，虽然可能是因为有的电影中评和差评数量可能不足500条，以及爬的时间过程会被豆瓣封一段时间ip，但短评数量依然相差有点多。所以后面打算再爬一次，为了避免以后由于封ip中断爬虫导致需要重新爬取的问题，所以事先存下短评的网页链接，根据网页链接和评论者的链接判断是否爬过这条评论，这样就可以在爬取时防止爬取已经爬过的短评，可以多次运行爬虫爬取未爬过的数据。

已爬取数据示例:
![./images/Snipaste_2019-11-06_18-59-46.jpg]()

commenters.py —— 用于爬取每个短评的评论者的信息。

每位评论者要爬取的具体信息：
![./images/Snipaste_2019-11-06_19-07-33.jpg]()

![./images/Snipaste_2019-11-06_19-08-28.jpg]()

CommenterInfoItem 设置：
![./images/Snipaste_2019-11-06_19-08-00.jpg]()

在爬取过程中查看log文件可以发现爬取评论者信息时的一些问题，比如 该用户已经主动注销帐号、没有填写常居地、账号根据用户管理细则已被永久停用等。对于常居地和注册时间爬取有问题的评论者可以以"未知"等默认值填充字段，但如果账号已注销或被停用则不储存评论者信息。

已爬取数据示例:
![]()